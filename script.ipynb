{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import time\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick Database: \n",
      "1 - Amazon Books\n",
      "2 - Best Buy\n",
      "3 - K-mart\n",
      "4 - Nike\n",
      "5 - Generic\n",
      "6 - Homework Example\n",
      "7 - Amazon Food\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "print(\"Pick Database: \")\n",
    "print(\"1 - Amazon Books\")\n",
    "print(\"2 - Best Buy\")\n",
    "print(\"3 - K-mart\")\n",
    "print(\"4 - Nike\")\n",
    "print(\"5 - Generic\")\n",
    "print(\"6 - Homework Example\")\n",
    "print(\"7 - Amazon Food\")\n",
    "\n",
    "db = str(input(\"Please input the desired database\"))\n",
    "min_support = float(input(\"Please input the desired minimum support\"))\n",
    "min_confidence = float(input(\"Please input the desired minimum confidence\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected database: amazon_Food\n",
      "Selected minimum support: 0.2\n",
      "Selected minimum confidence: 0.2\n"
     ]
    }
   ],
   "source": [
    "dic = {\n",
    "    '1': \"amazon_book\",\n",
    "    '2': \"bestbuy\",\n",
    "    '3': 'kmart',\n",
    "    '4': 'nike',\n",
    "    '5': 'generic',\n",
    "    '6': 'class_example',\n",
    "    '7': 'amazon_Food'\n",
    "}\n",
    "\n",
    "table = dic[db]\n",
    "print(\"Selected database:\", table)\n",
    "print(\"Selected minimum support:\", min_support)\n",
    "print(\"Selected minimum confidence:\", min_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute Force Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute Force\n",
      "Runtime: 249.1199414730072 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def brute_force(table):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(f'{table}.db')\n",
    "\n",
    "    # Execute SELECT query to retrieve data from the table\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    \n",
    "    df['Transactions'] = df['Transactions'].str.split(', ').apply(set)\n",
    "\n",
    "    def generate_combinations(lst, k):\n",
    "        return set(combinations(lst, k))\n",
    "\n",
    "    def calculate_min_support(data, itemset):\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            if set(item for tpl in itemset for item in tpl).issubset(row):\n",
    "                count += 1\n",
    "        return count / df.shape[0]\n",
    "\n",
    "    # Generate all frequent 1-itemset\n",
    "    def first_pass(df, support):\n",
    "        item = set()\n",
    "        max_transaction_length = 1\n",
    "        # Find all unique items in the transaction list and max transaction length\n",
    "        for index, row in df.iterrows():\n",
    "            for i in row['Transactions']:\n",
    "                item.add(tuple([i]))\n",
    "                max_transaction_length = max(len(row['Transactions']), max_transaction_length)\n",
    "\n",
    "        itemsets = []\n",
    "\n",
    "        # Add all 1-itemset and support to the itemsets\n",
    "        for i in item:\n",
    "            support = calculate_min_support(df['Transactions'].values, tuple([i]))\n",
    "            itemsets.append([[i], support])\n",
    "\n",
    "        return item, itemsets, max_transaction_length\n",
    "\n",
    "    # Generate all k-itemset, where k is between 2 and the max transaction length\n",
    "    def second_pass(df, item, itemsets, support, max_transaction_length):\n",
    "        k = 2\n",
    "        \n",
    "        # The stop condition for brute force will be the k-itemset where k is the max transaction length\n",
    "        while k <= max_transaction_length:\n",
    "\n",
    "            # Generate and add all k-itemset and support to the itemsets\n",
    "            all_combinations = generate_combinations(item, k)\n",
    "            for i in all_combinations:\n",
    "                support = calculate_min_support(df['Transactions'].values, i)\n",
    "                itemsets.append([list(i), support])\n",
    "            k += 1\n",
    "            \n",
    "        return itemsets\n",
    "\n",
    "    def filter_itemsets(itemsets, min_support):\n",
    "        res = []\n",
    "        for i in itemsets:\n",
    "            subset, support = i\n",
    "            if support >= min_support:\n",
    "                res.append(i)\n",
    "        return res\n",
    "\n",
    "    # Begin timer\n",
    "    start = time.time()\n",
    "\n",
    "    item, itemsets, max_transaction_length = first_pass(df, min_support)\n",
    "    if max_transaction_length >= 2:\n",
    "        itemsets = second_pass(df, item, itemsets, min_support, max_transaction_length)\n",
    "    frequent_itemsets = filter_itemsets(itemsets, min_support)\n",
    "\n",
    "    # Create all subset for an item in the frequent itemset\n",
    "    # If item = A, B and item length is 2\n",
    "    # The subsets would be A, B, AB\n",
    "    def find_subset(item, item_length):\n",
    "        subsets = []\n",
    "        for i in range(1, item_length + 1):\n",
    "            subsets.extend(combinations(item, i))\n",
    "        return subsets\n",
    "\n",
    "    # Generate all association rules from the frequent itemset\n",
    "    def association_rules(frequent_itemsets, min_confidence):\n",
    "        rules = list()\n",
    "        dic = {}\n",
    "\n",
    "        # Convert the frequent itemset into a dictionary\n",
    "        for item in frequent_itemsets:\n",
    "            key = frozenset(item[0])\n",
    "            value = item[1]\n",
    "            dic[key] = value\n",
    "\n",
    "        # Generate all possible {X} -> {Y} and checks if {XY} exist to compute confidence\n",
    "        for item, support in dic.items():\n",
    "            item_length = len(item)\n",
    "            if item_length > 1:\n",
    "                subsets = find_subset(item, item_length)\n",
    "                for X in subsets:\n",
    "                    Y = set(item).difference(X)\n",
    "                    if Y:\n",
    "                        X = frozenset(X)\n",
    "                        XY = X | frozenset(Y)\n",
    "\n",
    "                        # Checks if XY and X exists in the dictionary of frequent items\n",
    "                        if XY in dic and X in dic:\n",
    "                            confidence = dic[XY] / dic[X]\n",
    "\n",
    "                            # Add {X} -> {Y} that have confidence above the min_confidence \n",
    "                            if confidence >= min_confidence:\n",
    "                                rules.append((X, Y, confidence))\n",
    "        return rules\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "    # Stop timer\n",
    "    end = time.time()\n",
    "\n",
    "    # Write all itemsets and support to textfile, bruteforce will add all possible itemset combinations\n",
    "    with open(f\"{table}_bruteforce_unfilteredfreqlist_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "        \n",
    "        for i in itemsets:\n",
    "            item, supp = i\n",
    "            f.write(f\"{item}: {supp}\\n\")\n",
    "\n",
    "    # Write all frequent itemsets and support to textfile\n",
    "    with open(f\"{table}_bruteforce_freqlist_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "        \n",
    "        for i in frequent_itemsets:\n",
    "            item, supp = i\n",
    "            f.write(f\"{item}: {supp}\\n\")\n",
    "\n",
    "    # Write all association rules and confidence to textfile\n",
    "    with open(f\"{table}_bruteforce_association_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "        \n",
    "        for i in rules:\n",
    "            X, Y, conf = i\n",
    "            f.write(f\"{X} -> {Y} : {conf}\\n\")\n",
    "\n",
    "    print(\"Brute Force\")\n",
    "    print(\"Runtime:\", end - start, \"seconds\\n\")\n",
    "    return\n",
    "\n",
    "try:\n",
    "    brute_force(table)\n",
    "except:\n",
    "    print(\"There is an error, please try another combination of support, confidence, and df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori Algorithm Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Transaction_ID                                       Transactions\n",
      "0                1  Wonderful Pistachios No Shells, PASTA RONI Qua...\n",
      "1                2                                Frito-Lay Party Mix\n",
      "2                3  Quaker Instant Oatmeal Express Cups, Jack Link...\n",
      "3                4  GoGo squeeZ Fruit on the Go, Nature Valley Cru...\n",
      "4                5  Bumble Bee Chunk Light Tuna In Water, HORMEL C...\n",
      "5                6  Mott Fruit Flavored Snacks, Nature Valley Crun...\n",
      "6                7  Kelloggs Cold Breakfast Cereal, Chef Boyardee ...\n",
      "7                8  TWIZZLERS Twists Strawberry Flavored Licorice ...\n",
      "8                9  GoGo squeeZ Fruit on the Go, Maruchan Ramen Ch...\n",
      "9               10  Kelloggs Cold Breakfast Cereal, TWIZZLERS Twis...\n",
      "10              11  Quaker Instant Oatmeal, Quaker Instant Oatmeal...\n",
      "11              12  SpaghettiOs Canned Pasta with Meatballs, Jack ...\n",
      "12              13  Chef Boyardee Beef Ravioli, Premier Liquid Pro...\n",
      "13              14  Frito-Lay Party Mix, Glico Pocky, Kelloggs Col...\n",
      "14              15  Pop-Tarts Toaster Pastries, Glico Pocky, Slim ...\n",
      "15              16  Nissin Chow Mein Teriyaki, Glico Pocky, Premie...\n",
      "16              17  Bumble Bee Chunk Light Tuna In Water, Kraft Ea...\n",
      "17              18  GoGo squeeZ Fruit on the Go, Glico Pocky, Bumb...\n",
      "18              19  Wonderful Pistachios No Shells, BUMBLE BEE Sna...\n",
      "19              20  Nissin Chow Mein Teriyaki, Quaker Instant Oatm...\n",
      "Apriori Algorithm\n",
      "Runtime: 0.00796198844909668 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def apriori_algo(table):\n",
    "    # Intialize db as a dataframe\n",
    "    conn = sqlite3.connect(f'{table}.db')\n",
    "\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    df['Transactions'] = df['Transactions'].str.split(', ').apply(set)\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    # Generate all k-itemsets using the combinations from [k-1]-itemsets starting from k=3\n",
    "    def generate_new_combinations(prev_frequent_item, k):\n",
    "        prev_frequent_item_list = list(prev_frequent_item)  # Convert set to list\n",
    "        new_combinations = set()\n",
    "        # Iterate over unique pairs of combinations\n",
    "        for i, item_1 in enumerate(prev_frequent_item_list):\n",
    "            for item_2 in prev_frequent_item_list[i + 1:]:\n",
    "                # Prunes k-itemset that are not frequent in the previous level\n",
    "                if all(x == y for x, y in zip(item_1[:-1], item_2[:-1])):\n",
    "                    \n",
    "                    # Create the k-itemset from the frequent \n",
    "                    union_set = tuple(sorted(set(item_1 + item_2)))\n",
    "                    if len(union_set) == k:\n",
    "                        new_combinations.add(union_set)\n",
    "        return new_combinations\n",
    "\n",
    "    # Generates all 2-itemsets from the frequent 1-itemset\n",
    "    def generate_combinations(item, k):\n",
    "        return set(combinations(item, k))\n",
    "\n",
    "    # Calcualtes the support for a given itemset by dividing the frequency of the itemset in the transaction list by the total number of transactions\n",
    "    def calculate_support(data, itemset):\n",
    "        count = 0\n",
    "        for row in data:\n",
    "            if set(item for tpl in itemset for item in tpl).issubset(row):\n",
    "                count += 1\n",
    "        return count / df.shape[0]\n",
    "\n",
    "    # Generate all frequent 1-itemset\n",
    "    def first_pass(df, min_support):\n",
    "        item = set()\n",
    "        max_transaction_length = 1\n",
    "\n",
    "        # Find all unique items in the transaction list\n",
    "        for index, row in df.iterrows():\n",
    "            for i in row['Transactions']:\n",
    "                item.add(tuple([i]))\n",
    "                max_transaction_length = max(len(row['Transactions']), max_transaction_length)\n",
    "\n",
    "        frequent_itemsets = []\n",
    "        remove = []\n",
    "\n",
    "        # Checks if the unique items are frequent\n",
    "        for i in item:\n",
    "            support = calculate_support(df['Transactions'].values, tuple([i]))\n",
    "            if support < min_support:\n",
    "                remove.append(i)\n",
    "            else:\n",
    "                frequent_itemsets.append([[i], support])\n",
    "\n",
    "        # Remove infrequent items \n",
    "        item.difference_update(remove)\n",
    "\n",
    "        return item, frequent_itemsets, max_transaction_length\n",
    "\n",
    "    # Generate all frequent k-itemset, where k >= 2\n",
    "    def second_pass(df, item, frequent_itemsets, min_support, max_transaction_length):\n",
    "        k = 2\n",
    "        while True:\n",
    "            # Generate k-itemsets\n",
    "            if k == 2:\n",
    "                all_combinations = generate_combinations(item, k)\n",
    "            else:\n",
    "                all_combinations = generate_new_combinations(next_combinations, k)\n",
    "            next_combinations = set()\n",
    "            # For every k-itemset, check if they are frequent\n",
    "            for i in all_combinations:\n",
    "                support = calculate_support(df['Transactions'].values, i)\n",
    "                if support >= min_support:\n",
    "                    frequent_itemsets.append([list(i), support])\n",
    "                    # Adds only the frequent \n",
    "                    next_combinations.add(i)\n",
    "            \n",
    "            # If there are no more frequent itemsets, we know to stop looking at the next level\n",
    "            if len(next_combinations) == 0:\n",
    "                break\n",
    "            \n",
    "            k += 1\n",
    "\n",
    "        return frequent_itemsets\n",
    "\n",
    "    # Generate frequent 1-itemsets \n",
    "    item, frequent_itemsets, max_transaction_length = first_pass(df, min_support)\n",
    "\n",
    "    # Generate frequent k-itemsets \n",
    "    frequent_itemsets = second_pass(df, item, frequent_itemsets, min_support, max_transaction_length)\n",
    "\n",
    "    # Create all subset for an item in the frequent itemset\n",
    "    # If item = A, B and item length is 2\n",
    "    # The subsets would be A, B, AB\n",
    "    def find_subset(item, item_length):\n",
    "        subsets = []\n",
    "        for i in range(1, item_length + 1):\n",
    "            subsets.extend(combinations(item, i))\n",
    "        return subsets\n",
    "\n",
    "    # Generate all association rules from the frequent itemset\n",
    "    def association_rules(frequent_itemsets, min_confidence):\n",
    "        rules = list()\n",
    "        dic = {}\n",
    "\n",
    "        # Convert the frequent itemset into a dictionary\n",
    "        for item in frequent_itemsets:\n",
    "            key = frozenset(item[0])\n",
    "            value = item[1]\n",
    "            dic[key] = value\n",
    "\n",
    "        # Generate all possible {X} -> {Y} and checks if {XY} exist to compute confidence\n",
    "        for item, support in dic.items():\n",
    "            item_length = len(item)\n",
    "            if item_length > 1:\n",
    "                subsets = find_subset(item, item_length)\n",
    "                for X in subsets:\n",
    "                    Y = set(item).difference(X)\n",
    "                    if Y:\n",
    "                        X = frozenset(X)\n",
    "                        XY = X | frozenset(Y)\n",
    "\n",
    "                        # Checks if XY and X exists in the dictionary of frequent items\n",
    "                        if XY in dic and X in dic:\n",
    "                            confidence = dic[XY] / dic[X]\n",
    "\n",
    "                            # Add {X} -> {Y} that have confidence above the min_confidence \n",
    "                            if confidence >= min_confidence:\n",
    "                                rules.append((X, Y, confidence))\n",
    "        return rules\n",
    "\n",
    "    # Generate all association rules above the minimum confidence\n",
    "    rules = association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "    # Stop timer\n",
    "    end = time.time()\n",
    "\n",
    "    # Write all frequent itemsets and support to textfile\n",
    "    with open(f\"{table}_apriorialgo_freqlist_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "        \n",
    "        for i in frequent_itemsets:\n",
    "            item, supp = i\n",
    "            f.write(f\"{item}: {supp}\\n\")\n",
    "\n",
    "    # Write all association rules and confidence to textfile\n",
    "    with open(f\"{table}_apriorialgo_association_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "        \n",
    "        for i in rules:\n",
    "            X, Y, conf = i\n",
    "            f.write(f\"{X} -> {Y} : {conf}\\n\")\n",
    "\n",
    "    print(\"Apriori Algorithm\")\n",
    "    print(\"Runtime:\", end - start, \"seconds\\n\")\n",
    "    return \n",
    "\n",
    "try:\n",
    "    apriori_algo(table)\n",
    "except:\n",
    "    print(\"There is an error, please try another combination of support, confidence, and df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori Library Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori Algorithm from mlxtend\n",
      "Runtime: 0.010971546173095703 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christopher\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def apriori_library(table):\n",
    "    \n",
    "    # Intialize db as a dataframe\n",
    "    conn = sqlite3.connect(f'{table}.db')\n",
    "\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Start timer\n",
    "    start = time.time()\n",
    "\n",
    "    # One hot encode the unique items\n",
    "    new_df = df['Transactions'].str.get_dummies(', ')\n",
    "\n",
    "    # Generate frequent itemsets using mlxtend\n",
    "    frequent_itemsets = apriori(new_df, min_support, use_colnames=True)\n",
    "\n",
    "    # Generate association rules using mlxtend\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "    # Stop timer\n",
    "    end = time.time()\n",
    "\n",
    "    # Write all frequent itemsets and support to textfile\n",
    "    with open(f\"{table}_apriorilib_freqlist_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "\n",
    "        for index, row in frequent_itemsets.iterrows():\n",
    "            f.write(f\"{row['itemsets']}: {row['support']}\\n\")\n",
    "\n",
    "    # Write all association rules and confidence to textfile\n",
    "    with open(f\"{table}_apriorilib_association_output.txt\", \"w\") as f:\n",
    "        f.write(f\"min_support: {min_support}, min_confidence: {min_confidence}\\n\")\n",
    "\n",
    "        for index, rule in rules.iterrows():\n",
    "            f.write(f\"{rule['antecedents']} -> {rule['consequents']}: {rule['confidence']}\\n\")\n",
    "\n",
    "    print(\"Apriori Algorithm from mlxtend\")\n",
    "    print(\"Runtime:\", end - start, \"seconds\\n\")\n",
    "    return\n",
    "\n",
    "try:\n",
    "    apriori_library(table)\n",
    "except:\n",
    "    print(\"There is an error, please try another combination of support, confidence, and df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5acc429cc2e58fa062afccf6e589a7de30fbc8a71cba6007bd633148c4825db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
